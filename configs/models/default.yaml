# Default model configuration

envs:
  # Example environment variables
  # HUGGING_FACE_HUB_TOKEN: "your_token_here"
  VLLM_ROCM_USE_AITER: 1

vllm_server_args:
  # Arguments for the vLLM server
  quantization: auto
  kv-cache-dtype: auto
  # Add other vLLM server arguments here
  # example:
  # max_model_len: 1024
  # gpu-memory-utilization: 0.95
  # max-num-batched-token: 8192
  # swap-space: 16
  # block-size: 64
  # no-enable-prefix-caching: true
  # async-scheduling: true

tp_specific_overrides:
  1: # tp=1
  2: # tp=2
  4: # tp=4
  8:
    # tp=8

compilation_config:
  cudagraph_mode: "FULL_AND_PIECEWISE"
