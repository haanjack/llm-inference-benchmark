# Default model configuration

envs:
  # Example environment variables
  # HUGGING_FACE_HUB_TOKEN: "your_token_here"
  VLLM_ROCM_USE_AITER: 1

vllm_server_args:
  # Arguments for the vLLM server
  quantization: auto
  kv-cache-dtype: auto
  # Add other vLLM server arguments here
  # example:
  # max_model_len: 1024
  # gpu-memory-utilization: 0.95
  # max-num-batched-token: 8192
  # swap-space: 16
  # no-enable-prefix-caching: true
  # async-scheduling: true

parallel:
  1: # tp=1
  2:
    # tp=2
    block-size: 64
    enable-expert-parallel: true
  4:
    # tp=4
    block-size: 64
    enable-expert-parallel: true
  8:
    # tp=8
    block-size: 64
    enable-expert-parallel: true

compilation_config:
  cudagraph_mode: "FULL_AND_PIECEWISE"
